We observe that Dynamic Frontier (DF) Leiden outperforms both Naive-dynamic (ND) and Delta-screening (DS) Leiden, as shown in Section \ref{sec:performance-comparison}. Therefore, we focus most of our discussion below on how to extend the DF approach to the Leiden algorithm. The procedure the extend the ND and DS approaches to the Leiden algorithm are quite similar\ignore{, and are discussed at the end of this section}.




\subsection{No continued passes}
\label{sec:no-continued-passes}

A straightforward application of the DF approach to the Leiden algorithm involves processing incrementally identified affected vertices during the local-moving phase of the algorithm, refining the obtained communities in the subsequent phase, aggregating the refined communities into a super-vertex graph (where each refined community is collapsed into a super-vertex), and repeating this process until convergence is reached. However, for small batch updates, convergence may occur after just one pass of the algorithm, at which point the algorithm would terminate. This results in suboptimal communities with low modularity, as the communities generated from the refinement phase did not have the opportunity to hierarchically merge and form tightly-knit groups with dense internal connections and sparse inter-community links. An example of the communities returned in the absence of further processing is shown in Figure \ref{fig:subrefine-stages--01}, given a batch update. It is important to note that this issue does not arise in the absence of the refinement phase --- it occurs specifically when refinement is applied, as it forces the communities identified during the local-moving phase to be divided into smaller sub-communities. 

\input{src/fig-subrefine-stages}




\subsection{Full Refine method}
\label{sec:full-refine-method}

Despite the above mentioned issues, we\ignore{still} aim to retain the refinement phase in the Leiden algorithm due to its beneficial properties, such as preventing the formation of poorly connected or even internally disconnected communities \cite{com-traag19}. To this end, unlike DF Louvain \cite{sahu2024shared}, we do not stop the algorithm after the first pass, but rather, run the algorithm until convergence occurs in a subsequent local-moving phase, where all vertices are processed, not just the affected ones. We refer to this method as \textbf{Full Refine}. Figure \ref{fig:subrefine-stages--02} shows an example of the communities returned with the \textit{Full Refine} method.




\subsection{Subset Refine method}
\label{sec:subset-refine-method}

Nevertheless, for small batch updates, only a few communities may be impacted, and only those require refinement. To identify these communities, we track vertices that migrate between communities during the local-moving phase and mark both the source and target communities as changed, i.e., to be refined --- these communities may have altered sub-community structures. However, the community membership ID assigned to each vertex from the local-moving phase may not always be consistent; for example, community $c$ might not contain a vertex with ID $c$, as it could have migrated to another community. This inconsistency could cause issues during the refinement phase, where each vertex, in the communities to be refined, must initially belong to its own community. To understand this, consider an example. Say we break up the community, the vertex with ID $c$ belong to (for refinement), but do not break up the community $c$, vertex $c$ could continue to stay as an isolated community, while not being connected to any of the vertices in community $c$. Worse still, it is possible that two separate communities with the same ID $c$ are formed, while not being connected. It should be noted that this problem does not occur if we refine or break up all communities, but rather arises only if we refine a subset of communities. To resolve this, we must renumber communities such that the vertex with ID $c$ is part of community $c$, or in other words, by renumbering each community $c$ to an new community ID $i$, where $i$ is one of the constituent vertices of community $c$. In addition, we must accordingly adjust the total community edge weights and changed community flags, since these are populated based on the old community IDs.
% Legit operations due to a batch update

Next, we consider the impact of batch updates on the communities requiring refinement. Edge deletions within a community can cause it to split. However, if a community is isolated, the local-moving phase cannot find better community assignments for any of the vertices belonging to the community. In the extreme case, edge deletions within a community can also cause a community to be internally disconnected, if vertices within the community do not change their community assignments. This scenario can occur even if the community is not isolated. In order to address both concerns, refinement is the only way for such communities to split. Since community migrations do not occur, these communities would not be marked for refinement. Accordingly, for edge deletions in the batch update belonging to the same community, we mark the community as changed --- ensuring it is refined after the local-moving phase. Similarly, edge insertions affecting different parts of a community might also lead to a split, and must therefore be refined. Accordingly, for edge insertions in the batch update belonging to the same community, we mark the community as changed. Figure \ref{fig:community-split} shows an example of how these corner cases might occur with isolated communities, which may cause the community to split. We refer to this method of selective refinement of communities obtained from the local-moving phase of the algorithm, considering both vertex migration and the batch update, as \textbf{Subset Refine}. Figure \ref{fig:subrefine-stages--03} shows an example of the communities returned with the \textit{Subset Refine} method. Note that only a subset of the communities are processed into subcommunities.
% Edge deletions and insertions within a community may cause it to split. However, if the community is isolated, the local-moving phase will not be able to partition such communities --- since there are no better communities to move to, for any of the member vertices. In order to address this issue, such communities must be refined. Additionally, refining non-isolated communities with intra-community edge deletions and insertions facilitates splitting of such communities.

Note that selective refinement is applied only in the first pass of the Leiden algorithm, while all communities are refined in subsequent passes. This is because later passes work on smaller super-vertex graphs, making selective refinement less beneficial. Additionally, tracking communities across the hierarchy would be costly, and the time saved by selective refinement in these smaller graphs wouldn't justify the overhead.
% Note that selective refinement is performed only in the first pass of Leiden algorithm --- in the remaining passes, all the communities are refined. This is because further passes operate on super-vertex graphs, which are relatively small in size, and also because the tracking the communities across the hierarchy would be expensive, and time savings obtained due to selective refinement in super-vertex graphs would not be worth the overhead.

\input{src/fig-community-split}




\subsection{Optimized Aggregation method}
\label{sec:optimized-aggregation-method}

However, the above selective refinement method hardly improves the performance of the DF Leiden, particularly for small batch updates. For these smaller updates, the aggregation phase remains a major bottleneck. This is mainly due to the selective refinement of communities, which results in a large difference in the sizes of communities to be aggregated --- the refined communities tend to be small, while the unrefined communities tend to be large. This creates a heavy workload for threads aggregating these unrefined communities into super-vertices. To ensure better load balancing, heavily loaded threads should be assigned minimal additional communities. Dynamic work scheduling can help, with each thread being assigned a smaller range of community IDs, or chunks. However, chunk sizes that are too small can introduce significant scheduling overhead. Through experimentation with chunk sizes from $1$ to $2048$ --- on large real-world graphs (given in Table \ref{tab:dataset-large}), with uniformly random batch updates consisting of $80\%$ edge insertions and $20\%$ edge deletions (see Section \ref{sec:batch-generation}), on batch sizes of $10^{-7}|E|$, $10^{-5}|E|$, and $10^{-7}|E|$, where $|E|$ is the number of edges in the original graph --- we find that a chunk size of $32$ provides the best overall performance for ND, DS, and DF Leiden; as shown in Figure \ref{fig:aggregation-adjust-chunksize}. We refer to this method as \textbf{Optimized Aggregation}. Figure \ref{fig:subrefine-optimize} illustrates the\ignore{mean} runtime of DF Leiden based on\ignore{the three methods mentioned,} \textit{Full Refine}, \textit{Subset Refine}, and \textit{Optimized Aggregation} methods, on large graphs with batch updates of size $10^{-7}|E|$ to $0.1|E|$.

\input{src/fig-aggregation-adjust-chunksize}
\input{src/fig-subrefine-optimize}

The pseudocode for \textit{Optimized Aggregation} based ND, DS, and DF Leiden, which we from here on refer to simply as ND, DS, and DF Leiden, respectively, is presented in Algorithms \ref{alg:naive}, \ref{alg:delta}, and \ref{alg:frontier}, with detailed explanations in Sections \ref{sec:our-naive}, \ref{sec:our-delta}, \ref{sec:our-frontier}, respectively. In the first pass of Leiden algorithm, we process the vertices identified as affected by the DS and DF approaches, initializing the community membership of each vertex based on the membership obtained from the previous snapshot of the graph. In the following passes, all super-vertices are marked as affected and processed according to the Leiden algorithm \cite{sahu2024shared}. Similar to DF Louvain \cite{sahu2024dflouvain}, we use the weighted degrees of vertices and the total edge weights of communities as auxiliary information \cite{sahu2024dflouvain}\ignore{, as illustrated in Figure \ref{fig:about-auxiliary}}.




\subsection{Implementation details}
\label{sec:implementation-details}

We use an asynchronous version of Leiden, where threads independently process different graph parts, enabling faster convergence but increasing variability in the final result. Each thread has its own hashtable to track delta-modularity during the local-moving and refinement phases and the total edge weight between super-vertices in the aggregation phase. Optimizations include OpenMP's dynamic loop schedule, limiting iterations to $20$ per pass, a tolerance drop rate of $10$ starting at $0.01$, vertex pruning, using parallel prefix sums and preallocated Compressed Sparse Row (CSR) data structures for super-vertex graphs and community vertices, and using fast, collision-free per-thread hashtables\ignore{for all algorithm phases} \cite{sahu2024fast}.

For simplicity, we do not skip the aggregation phase of Leiden algorithm, even if a small number of communities are being merged together, unlike the original implementation of Static Leiden \cite{sahu2024fast}. This allows us to maintain a high quality of obtained communities while incurring only a minor increase in runtime across Static, ND, DS, and DF Leiden. Additionally, we employ the refine-based variation of Leiden, where the community labels of super-vertices are determined by the refinement phase rather than the local-moving phase (i.e., move-based)\ignore{\cite{sahu2024fast}}, as the latter would not permit community splits in the scenarios outlined in Section \ref{sec:subset-refine-method}.


\subsection{Time and Space complexity}

The time complexity of ND, DS, and DF Leiden is the same as Static Leiden, i.e., $O(L|E^t|)$, where $L$ is the total number of iterations performed. However, the cost of local-moving and refinement phases in the first pass is reduced, and depends both on the size and nature of the batch update. The space complexity of our algorithms is also the same as Static Leiden, i.e., $O(T|V^t| + |E^t|)$, where $T$ represents the number of threads used ($T|V^t|$ is the space used by per-thread collision-free hashtables \cite{sahu2024fast}).
% \input{src/fig-about-auxiliary}
