\subsection{Experimental setup}
\label{sec:setup}

\subsubsection{System}
\label{sec:system}

In our experiments, we employ a server equipped with an x86-based 64-bit AMD EPYC-7742 processor. It operates at a clock frequency of $2.25$ GHz and is paired with $512$ GB of DDR4 system memory. Each core features a $4$ MB L1 cache, a $32$ MB L2 cache, and a shared L3 cache of $256$ MB. The server runs on Ubuntu 20.04.


\subsubsection{Configuration}
\label{sec:configuration}

We utilize 32-bit unsigned integers for vertex IDs and 32-bit floating-point for edge weights. For computation involving floating-point aggregation, like total edge weights and modularity, we switch to 64-bit floating-point. Affected vertices and changed communities are represented by 8-bit integer vectors. During the local-moving, refinement, and aggregation phases, we employ OpenMP's dynamic scheduling with a chunk size of $2048$ for load balancing. However, for the aggregation phase in Naive-dynamic (ND), Delta-screening (DS), and Dynamic Frontier (DF) Leiden, a chunk size of $32$ is used. We run all parallel implementations on $64$ threads and compile using GCC 9.4 with OpenMP 5.0.
% Key parameters include an iteration tolerance $\tau$ of $10^{-2}$ for the local-moving phase, a \textit{TOLERANCE\_DECLINE\_FACTOR} of $10$ for threshold scaling optimization \cite{lu2015parallel}, a \textit{MAX\_ITERATIONS} of $20$ for the local-moving phase per pass, and a \textit{MAX\_PASSES} of $10$ \cite{sahu2024fast}.

\subsubsection{Dataset}
\label{sec:dataset}

We conduct experiments on $12$ large real-world graphs with random batch updates, listed in Table \ref{tab:dataset-large}, obtained from the SuiteSparse Matrix Collection. These graphs have between $3.07$ million and $214$ million vertices, and $25.4$ million to $3.80$ billion edges. All edges are undirected and weighted with a default weight of $1$. Since most publicly available real-world weighted graphs are small in size, we do not use them here --- although our algorithms can handle weighted graphs without modification. Further, existing temporal graphs in the SNAP repository \cite{snapnets} are mostly small\ignore{(with one exception)}, limiting their utility for studying our proposed parallel algorithms.

We also do not use SNAP datasets with ground-truth communities, as they are non-disjoint, while our work focuses on disjoint communities. Nevertheless, it is important to note that the goal of community detection is not necessarily to match the ground truth, as this may reflect a different aspect of network structure or be unrelated to the actual structure, and focusing solely on this correlation risks overlooking other meaningful community structures \cite{peel2017ground}.
% For experiments involving real-world dynamic graphs, we utilize five temporal networks sourced from the Stanford Large Network Dataset Collection \cite{snapnets}, detailed in Table \ref{tab:dataset}. Here, the number of vertices span from $24.8$ thousand to $2.60$ million, temporal edges from $507$ thousand to $63.4$ million, and static edges from $240$ thousand to $36.2$ million. With each graph, we ensure that all edges are undirected and weighted, with a default weight of $1$.

\input{src/tab-dataset-large}
% \input{src/tab-dataset}


\subsubsection{Batch generation}
\label{sec:batch-generation}

We take each base graph from Table \ref{tab:dataset-large} and generate random batch updates \cite{com-zarayeneh21}, comprising an $80\% : 20\%$ mix of edge insertions and deletions to emulate realistic batch updates, with each edge having a weight of $1$. All updates are undirected; for every edge insertion $(i, j, w)$, we also include $(j, i, w)$. To prepare the set of edges for insertion, we select vertex pairs with equal probability. For edge deletions, we uniformly delete existing edges. To simplify, no new vertices are added or removed from the graph. The batch size is measured as a fraction of the edges in the original graph, ranging from $10^{-7}$ to $0.1$ (i.e., $10^{-7}|E|$ to $0.1|E|$). For a billion-edge graph, this translates to a batch size ranging from $100$ to $100$ million edges. We employ five distinct random batch updates for each batch size and report the average across these runs. Note that dynamic graph algorithms are helpful for small batch updates in interactive applications. For large batches, it is usually more efficient to run the static algorithm.
% For experiments involving real-world dynamic graphs, we start by loading $90\%$ of each graph from Table \ref{tab:dataset}, and ensure all edges are weighted with a default weight of $1$ and are undirected by adding reverse edges. Subsequently, we load $B$ edges in $100$ batch updates. Here, $B$ denotes the desired batch size, specified as a fraction of the total number of temporal edges $|E_T|$ in the graph, and ensure the batch update is undirected.


\subsubsection{Measurement}
\label{sec:measurement}

We evaluate the runtime of each method on the entire updated graph, encompassing all stages: the local-moving phase, refinement phase, aggregation phase, initial and incremental marking of affected vertices, convergence detection, and any necessary intermediate steps.\ignore{We exclude memory allocation and deallocation times.} To mitigate the impact of noise, we follow the standard practice of running each experiment multiple times. We assume that the total edge weight of the graphs is known and can be tracked with each batch update. As our baseline, we use the most efficient multicore implementation of Static Leiden \cite{sahu2024fast}, which has been show to outperform even GPU-based implementations. Given that modularity maximization is NP-hard, all existing polynomial-time algorithms are heuristic; thus, we assess the optimality of our dynamic algorithms by comparing their convergence to the modularity score achieved by the static algorithm. Finally, none of the analyzed algorithms --- Static, ND, DS, and DF Leiden --- produce communities that are internally disconnected, so we omit this from our figures.

\input{src/fig-8020-runtime}
\input{src/fig-8020-modularity}
% \input{src/fig-temporal-summary}




\subsection{Performance Comparison}
\label{sec:performance-comparison}

We now evaluate the performance of our parallel implementations of ND, DS, and DF Leiden against Static Leiden \cite{sahu2024fast}, on large graphs with randomly generated batch updates. As detailed in Section \ref{sec:batch-generation}, these updates range in size from $10^{-7}|E|$ to $0.1|E|$, with $80\%$ edge insertions and $20\%$ edge deletions. Each batch update includes reverse edges to keep the graph undirected. Figure \ref{fig:8020-runtime--all} presents the execution time of each algorithm across individual graphs, while Figure \ref{fig:8020-runtime--mean} depicts overall runtimes using geometric mean for consistent scaling across differing graph sizes. In addition, Figure \ref{fig:8020-modularity--all} shows the modularity of the algorithms for individual graphs in the dataset, and Figure \ref{fig:8020-runtime--mean} presents the overall modularity of each of the algorithms, calculated using arithmetic mean.

%%
From Figure \ref{fig:8020-runtime--mean}, we observe that ND, DS, and DF Leiden achieve mean speedups of $1.37\times$, $1.47\times$, and $1.98\times$, respectively, when compared to Static Leiden. This speedup is higher on smaller batch updates, with ND, DS, and DF Leiden being on average $1.46\times$, $1.74\times$, and $3.72\times$, respectively, when compared to Static Leiden, on batch updates of size $10^{-7}|E|$. Figure \ref{fig:8020-runtime--all} shows that ND, DS, and DF Leiden offer moderate speedups over Static Leiden on web graphs and social networks --- while on road networks (with low average degree), ND Leiden marginally outperforms Static Leiden, and DS and DF Leiden perform significantly better. This is due to DS and DF Leiden processing significantly fewer affected vertices/communities on road networks. On protein k-mer graphs, DS Leiden offers only a slight improvement. However, on small batch updates, DF Leiden outperforms Static Leiden by a large margin.

In terms of modularity, as Figures \ref{fig:8020-modularity--mean} and \ref{fig:8020-modularity--all} show, ND, DS, and DF Leiden achieve communities with approximately the same modularity as Static Leiden. However, on social networks, ND, DS, and DF Leiden moderately outperform Static Leiden. This is likely due to social networks not having a strong enough community structure, and thus requiring more iterations to converge to a better community assignment. On such graphs, Static Leiden fails to attain better modularity due to it limiting the number of iterations per pass performed, in favor of faster runtimes. However, since ND, DS, and DF Leiden do not start from scratch, but from the community membership of Static Leiden, they are able to improve upon it further. Thus, on average, the modularity of communities from Static Leiden is slightly lower. However, we notice that modularity of communities identified by our algorithms do not differ by more than $0.002$ from Static Leiden, on average.

Also note in Figure \ref{fig:8020-runtime} that the runtime of Static Leiden increases with larger batch updates. This effect is primarily due to the random batch updates disrupting the original community structure, requiring Static Leiden to perform more iterations to achieve convergence, rather than simply being a result of the higher edge count in the graph. This disruption also accounts for the observed decline in modularity of the algorithms with larger batch sizes, as illustrated in Figure \ref{fig:8020-modularity}.

Let us now discuss why ND, DS, and DF Leiden exhibit only modest speedups over Static Leiden. Unlike Louvain algorithm, we must always run the refinement phase of Leiden algorithm to avoid badly connected and internally disconnected communities. Nonetheless, the refinement phase splits the communities obtained/updated from the local-moving phase into several smaller sub-communities. Stopping the passes early\ignore{, as done with DF Louvain,} leads to low modularity scores for ND/DS/DF Leiden because sufficiently large high-quality clusters have not yet formed\ignore{due to the refinement phase}. Further, ND/DS/DF Leiden can only reduce the runtime of the local-moving phase of the first pass of the Leiden algorithm. However, only about $37\%$ of the runtime in Static Leiden is spent in the local-moving phase of the first pass. These factors constrain the speedup potential of ND, DS, and DF Leiden over Static Leiden. Regardless, on large graphs with random batch updates, DF Leiden appears to be the dynamic community detection method of choice for tracking evolving communities.




%% TO REPHRASE

\subsection{Analysis of affected vertices and changed communities}

We now study the fraction of vertices marked as affected, and the communities marked as changed (used for keeping track of the subset of communities to be refined), by DS and DF Leiden on instances from Table \ref{tab:dataset-large}, with batch updates ranging from $10^-7|E|$ to $0.1|E|$. This tracking occurs only in the first pass of the Leiden algorithm, as detailed in Sections \ref{sec:subset-refine-method} and \ref{sec:optimized-aggregation-method}, and is shown in Figure \ref{fig:8020-affected}.
% We now study the fraction of vertices marked as affected, and the fraction of communities marked as changed (used for keeping track of the subset of communities to be refined), by DS and DF Leiden. We do this over the instances in Table \ref{tab:dataset-large}, on batch updates of size $10^-7|E|$ to $0.1|E|$. Note that we only keep track of affected vertices and changed communities in the first pass of Leiden algorithm, as mentioned in Sections \ref{sec:subset-refine-method} and \ref{sec:optimized-aggregation-method}, and this is what is measure in Figure \ref{fig:8020-affected}.

We notice from Figure \ref{fig:8020-affected} that DF Leiden marks fewer affected vertices and changed communities than DS Leiden, but their runtime difference is smaller, as many vertices marked by DS Leiden do not change their community labels, and converge quickly. However, as expected, performance of the algorithms decline with more affected vertices, as Figures \ref{fig:8020-runtime} and \ref{fig:8020-affected} show. Further, while a high number of changed communities increases refinement costs in the first pass, the cost of subsequent passes\ignore{is more or less constant, and} only depends on the graph's nature.
% We notice from Figure \ref{fig:8020-affected} that DF Leiden marks a significantly smaller fraction of vertices as affected and a decently smaller fraction of communities as changed, compared to DS Leiden. The runtime of these algorithms, however, do not differ in this ratio as DS Leiden marks entire communities of vertices as affected but many such affected vertices may not really undergo a change in their community label. So, a large fraction of such affected vertices converge in only one iteration.  Also, as Figures \ref{fig:8020-runtime} and \ref{fig:8020-affected} show, the performance of our algorithms drops as more vertices are marked as affected. In addition, while, a large number of changed communities tend to proportionally increase the cost of refinement phase in the first pass of the algorithm, the cost of the remaining passes is more of less constant, and only depends on the nature of the updated graph.

\input{src/fig-8020-affected}




\subsection{Scalability}

Finally, we study the strong-scaling behavior of ND, DS, and DF Leiden, and compare it with Static Leiden. To do this, we fix the batch size at $10^{-3} |E|$, vary the number of threads in use from $1$ to $64$, and measure the speedup of each algorithm with respect to its sequential version.

As shown in Figure \ref{fig:8020-scaling}, ND, DS, and DF Leiden obtain a speedup of $10.2\times$, $9.9\times$, and $9.0\times$ respectively at $32$ threads\ignore{(with respect to sequential execution)}; with their speedup increasing at a mean rate of $1.59\times$, $1.58\times$, and $1.55\times$, respectively, for every doubling of threads. At $64$ threads, NUMA affects the performance of our algorithms. In addition, increasing the number of threads makes the behavior of the algorithm less asynchronous, i.e., similar to the Jacobi iterative method, further contributing to the performance drop.

\input{src/fig-8020-scaling}




% \su{Finding densest community}
% If quality corresponds to modularity, one can pick the community with the largest modularity score to be the one of highest quality (densest subgraph).




% \subsection{Stability of communities identified}

% Intuitively, if the graphs $G^t$ and $G^{t'}$ are identical for some $t$ and $t'$, we expect DF Leiden to produce the same communities for $G^t$ and $G^{t'}$. We refer to this property of a dynamic algorithm as its stability, measured as the percentage of vertices that agree on the community label across two identical graphs. Vertices within weak community structures tend to be unstable, as they may connect to multiple communities with similar strength.

% To measure the stability of DF Leiden, we proceed as follows. Let $G$ be an initial graph. We generate a batch update of size $10^{-7} |E|$ to $0.1 |E|$ consisting of edge deletions to obtain the graph $G^1$. We then apply each of the above algorithms on $G^1$ to identify the new communities. Subsequently, we create another batch of updates that inserts the edges deleted in the prior time step. This graph, $G^2$, is essentially the original graph $G$. We obtain the community labels of the vertices in the graph $G^2$ by appealing to the dynamic algorithms. Finally, we compare the community label of each vertex in the graphs $G$ and $G^2$. We observe that DF Leiden has a minimum of $99.70\%$ match, thus indicating that it is stable.

% In strong community structures, vertices tend to be stable and well-connected within their community. Weak community structures can result in unstable vertices, as they may connect to multiple communities with similar strength.




% \subsection{Further discussion}

% \subsubsection{Results on real-world dynamic graphs}

% We also evaluate the performance of our parallel implementations of Static, ND, DS, and DF Leiden on real-world dynamic graphs listed in Table \ref{tab:dataset}. These evaluations are performed on batch updates ranging from $10^{-5}|E_T|$ to $10^{-3}|E_T|$ in multiples of $10$. For each batch size, as described in Section \ref{sec:batch-generation}, we load $90\%$ of the graph, add reverse edges to ensure all edges are undirected, and then load $B$ edges (where $B$ is the batch size) consecutively in $100$ batch updates. Figure \ref{fig:temporal-summary--runtime-overall} shows the overall runtime of each approach across all graphs for each batch size, while Figure \ref{fig:temporal-summary--modularity-overall} depicts the overall modularity of the obtained communities. Additionally, Figures \ref{fig:temporal-summary--runtime-graph} and \ref{fig:temporal-summary--modularity-graph} present the mean runtime and modularity of the communities obtained with each approach on individual dynamic graphs in the dataset.

% Figure \ref{fig:temporal-summary--runtime-overall} illustrates that ND Leiden is, on average, $1.09\times$ faster than Static Leiden for batch updates ranging from $10^{-5}|E_T|$ to $10^{-3}|E_T|$. In comparison, DS and DF Leiden exhibit average speedups of $1.20\times$ and $1.33\times$, respectively, over Static Leiden for the same batch updates. We now explain why ND, DS, and DF Leiden achieve only minor speedups over Static Leiden. Our experiments indicate that only about $20\%$ of the overall runtime of Static Leiden is spent in the local-moving phase of the first pass.\ignore{This is a significant decrease from the $37\%$ observed on large graphs with random batch updates.} Consequently, the speedup achieved by ND, DS, and DF Leiden relative to Static Leiden is limited. Furthermore, our observations suggest that while DF Leiden can reduce the time spent in the local-moving phase slightly more than ND Leiden, it incurs increased runtimes in the refinement and aggregation phases of the algorithm. This is likely because ND Leiden is able to optimize clusters more effectively than DF Leiden, which accelerates the refinement and aggregation phases for ND Leiden, albeit by a small margin. This explains the lower speedup of DF Leiden compared to ND Leiden on larger batch updates\ignore{of real-world dynamic graphs}.\ignore{Therefore, we recommend ND Leiden for real-world dynamic graphs.}

% Unlike with large graphs and random batch updates, on real-world dynamic graphs, we disable the aggregation tolerance $\tau_{agg}$. This results in a reduced portion of runtime spent in the local-moving phase of the first pass, which is the phase that can be minimized by the dynamic approaches. Specifically, we observe that only about $20\%$ of the overall runtime of Static Leiden is spent in the local-moving phase of the first pass.
